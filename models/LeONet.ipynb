{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2ba9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbbe7a-cf47-42f9-bed0-bce58dad7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python --version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf2fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the dataset\n",
    "dataset = pd.read_csv('./multiclass-4190-linear-input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3fc5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       linesCount  assignmentsCount  selectionStatementsCount  \\\n",
      "0              10                 1                         0   \n",
      "1              10                 1                         0   \n",
      "2              10                 1                         0   \n",
      "3              10                 1                         0   \n",
      "4               7                 0                         0   \n",
      "...           ...               ...                       ...   \n",
      "25135          34                 0                         0   \n",
      "25136          22                 1                         0   \n",
      "25137           9                 0                         0   \n",
      "25138          18                 1                         1   \n",
      "25139          49                 3                         1   \n",
      "\n",
      "       iterationStatementsCount  synchronizedStatementsCount  \\\n",
      "0                             1                            0   \n",
      "1                             1                            0   \n",
      "2                             1                            0   \n",
      "3                             1                            0   \n",
      "4                             0                            0   \n",
      "...                         ...                          ...   \n",
      "25135                         0                            0   \n",
      "25136                         0                            0   \n",
      "25137                         0                            0   \n",
      "25138                         0                            0   \n",
      "25139                         0                            0   \n",
      "\n",
      "       returnStatementsCount  switchCaseStatementsCount  tryStatementsCount  \\\n",
      "0                          0                          0                   0   \n",
      "1                          0                          0                   0   \n",
      "2                          0                          0                   0   \n",
      "3                          0                          0                   0   \n",
      "4                          0                          0                   0   \n",
      "...                      ...                        ...                 ...   \n",
      "25135                      1                          0                   1   \n",
      "25136                      2                          0                   1   \n",
      "25137                      2                          0                   1   \n",
      "25138                      1                          0                   2   \n",
      "25139                      2                          8                   2   \n",
      "\n",
      "       singleVariableDeclarationCount  variableDeclarationCount  ...  \\\n",
      "0                                   2                         6  ...   \n",
      "1                                   2                         6  ...   \n",
      "2                                   2                         6  ...   \n",
      "3                                   2                         6  ...   \n",
      "4                                   2                         4  ...   \n",
      "...                               ...                       ...  ...   \n",
      "25135                               4                        18  ...   \n",
      "25136                               4                         9  ...   \n",
      "25137                               2                         3  ...   \n",
      "25138                               3                         6  ...   \n",
      "25139                               3                        10  ...   \n",
      "\n",
      "       boxedTypesCount2  arrayCreationLevelsCount2  polyExprsCount2  \\\n",
      "0                     0                          1                0   \n",
      "1                     0                          1                0   \n",
      "2                     0                          1                0   \n",
      "3                     0                          1                0   \n",
      "4                     0                          0                0   \n",
      "...                 ...                        ...              ...   \n",
      "25135                 0                          0                0   \n",
      "25136                 0                          0                0   \n",
      "25137                 0                          0                0   \n",
      "25138                 0                          0                0   \n",
      "25139                 0                          0                0   \n",
      "\n",
      "       standaloneExprsCount2  elidedTypeArgumentsCount2  \\\n",
      "0                         27                         27   \n",
      "1                         27                         27   \n",
      "2                         27                         27   \n",
      "3                         27                         27   \n",
      "4                         20                         20   \n",
      "...                      ...                        ...   \n",
      "25135                     35                         35   \n",
      "25136                     35                         35   \n",
      "25137                     35                         35   \n",
      "25138                     35                         35   \n",
      "25139                     35                         35   \n",
      "\n",
      "       qualifiedExprNamesCount2  simpleExprNamesCount2  primaryExprsCount2  \\\n",
      "0                             4                     10                  11   \n",
      "1                             4                     10                  11   \n",
      "2                             4                     10                  11   \n",
      "3                             4                     10                  11   \n",
      "4                             6                      7                  11   \n",
      "...                         ...                    ...                 ...   \n",
      "25135                         7                     11                  14   \n",
      "25136                         7                     11                  14   \n",
      "25137                         7                     11                  14   \n",
      "25138                         7                     11                  14   \n",
      "25139                         7                     11                  14   \n",
      "\n",
      "       literalExprsCount2  cloneType  \n",
      "0                       3         t1  \n",
      "1                       3         t1  \n",
      "2                       3         t1  \n",
      "3                       3         t1  \n",
      "4                       1         t1  \n",
      "...                   ...        ...  \n",
      "25135                   3         t0  \n",
      "25136                   3         t0  \n",
      "25137                   3         t0  \n",
      "25138                   3         t0  \n",
      "25139                   3         t0  \n",
      "\n",
      "[25140 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad271cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1  0 ... 10 11  3]\n",
      " [10  1  0 ... 10 11  3]\n",
      " [10  1  0 ... 10 11  3]\n",
      " ...\n",
      " [ 9  0  0 ... 11 14  3]\n",
      " [18  1  1 ... 11 14  3]\n",
      " [49  3  1 ... 11 14  3]]\n",
      "['t1' 't1' 't1' ... 't0' 't0' 't0']\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(25140, 96)\n",
      "(25140,)\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# split dataset into X and Y\n",
    "# columns are 0-based index\n",
    "\n",
    "# x = dataset.iloc[:, 0:28].values # 0:28 means 0th-indexed column to 27th-indexed column - wildcardTypesCount\n",
    "# y = dataset.iloc[:, -1].values # -1 refers to the last column - cloneType\n",
    "\n",
    "x = dataset.iloc[:, 0:-1].values\n",
    "y = dataset.iloc[:, -1].values # -1 refers to the last column - cloneType\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(type(x))\n",
    "print(type(y))\n",
    "\n",
    "print(x.shape) # (25140, 48) = 25140 number of vectors in it and 48 number of scalars in each vector.\n",
    "print(y.shape) # (25140,) = 25140 number of scalers. Vector. So one-dimensional data.\n",
    "\n",
    "print(x.ndim)\n",
    "print(y.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c63e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]]\n",
      "(25140, 6)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, label_binarize, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# define One-Hot Encoding\n",
    "# encoder = OneHotEncoder(sparse = False)\n",
    "# transform data\n",
    "# y = encoder.fit_transform(y.reshape(-1,1))\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "# lb.fit(y) # target classes are arranged in alphabetically and not in our wish. So, for a class occurance 1 will be added, others will be 0.\n",
    "# print(lb.classes_) # ['mt3' 'st3' 't0' 't1' 'vst3' 'wt3']\n",
    "# y = lb.transform(y) # according to classes_, 1 will be added. Example, [0 0 0 1 0 0] or 3 represents 't1' class.\n",
    "# print(y)\n",
    "\n",
    "# This function makes it possible to compute this transformation for a fixed set of class labels known ahead of time.\n",
    "# label_binarize function preserves class order according to our wish!\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html#sklearn.preprocessing.label_binarize\n",
    "y = label_binarize(y, classes=['t1', 'vst3', 'st3', 'mt3', 'wt3', 't0']) # Example, [0 0 0 1 0 0] or 3 represents 'mt3' class.\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(y.ndim)\n",
    "\n",
    "# define LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# le.fit(y)\n",
    "# print(list(le.classes_))\n",
    "# y = le.transform(y)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a8d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdf776",
   "metadata": {},
   "source": [
    "# # Train-Test Split Method Begins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "017e64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17598, 96)\n",
      "(17598, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split the x and y dataset into the Training set and Test set while performing the data shuffling\n",
    "# https://www.kaggle.com/questions-and-answers/189700 - don't use train-test split, we'll use CV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4d0321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1)\n"
     ]
    }
   ],
   "source": [
    "# in Input object, shape = (24,) indicates that the expected input will be batches of 24-dimensional vectors.\n",
    "input_shape = (48,1)\n",
    "print(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81664445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function that creates and returns the subnetwork\n",
    "# def create_subnetwork(input_shape, initializer):\n",
    "#     input = keras.Input(shape = input_shape, name = 'subnetwork_input')\n",
    "#     x = keras.layers.Dense(units = 200, kernel_initializer = initializer, activation = 'relu')(input)\n",
    "#     x = keras.layers.Dropout(0.2)(x)\n",
    "#     x = keras.layers.Dense(units = 200, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "#     x = keras.layers.Dense(units = 200, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "#     x = keras.layers.Dropout(0.2)(x)\n",
    "#     x = keras.layers.Dense(units = 200, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "#     return keras.Model(name = 'subnetwork', inputs = input, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b377fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates and returns the subnetwork\n",
    "def create_subnetwork(input_shape, initializer):\n",
    "    input = keras.Input(shape = input_shape, name = 'subnetwork_input')\n",
    "    \n",
    "    x = keras.layers.Conv1D(filters = 6, strides = 1, kernel_size = 5, activation = 'relu')(input)\n",
    "    x = keras.layers.AveragePooling1D(pool_size = 2, strides = 2)(x)\n",
    "    \n",
    "    x = keras.layers.Conv1D(filters = 16, strides = 1, kernel_size = 5, activation = 'relu')(x)\n",
    "    x = keras.layers.AveragePooling1D(pool_size=2, strides = 2)(x)\n",
    "    \n",
    "    x = keras.layers.Conv1D(filters = 120, strides = 1, kernel_size = 5, activation = 'relu')(x)\n",
    "    \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = keras.layers.Dense(units = 420, activation = 'relu')(x)\n",
    "    \n",
    "#     x = keras.layers.Dense(units = y.shape[1], activation = 'softmax')(x)\n",
    "    \n",
    "    return keras.Model(name = 'subnetwork', inputs = input, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb8b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates and returns the comparatornetwork\n",
    "def create_comparatornetwork(input_vect_dim, initializer):\n",
    "    input = keras.Input(shape = (input_vect_dim,), name = 'comparatornetwork_input')\n",
    "    x = keras.layers.Dense(units = 200, kernel_initializer = initializer, activation = 'relu')(input)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(units = 100, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "    x = keras.layers.Dense(units = 50, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(units = 25, kernel_initializer = initializer, activation = 'relu')(x)\n",
    "    return keras.Model(name = 'comparatornetwork', inputs = input, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9885dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.initializers.initializers_v2.HeNormal object at 0x00000181EBD6FBB0>\n"
     ]
    }
   ],
   "source": [
    "# defining 'HeNormal' initializer\n",
    "initializer = keras.initializers.HeNormal(42)\n",
    "print(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526de38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 840)\n",
      "Model: \"LeONet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " left_input (InputLayer)        [(None, 48, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " right_input (InputLayer)       [(None, 48, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " subnetwork (Functional)        (None, 420)          262672      ['left_input[0][0]',             \n",
      "                                                                  'right_input[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 840)          0           ['subnetwork[0][0]',             \n",
      "                                                                  'subnetwork[1][0]']             \n",
      "                                                                                                  \n",
      " comparatornetwork (Functional)  (None, 25)          194625      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " classificationUnit (Dense)     (None, 6)            156         ['comparatornetwork[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 457,453\n",
      "Trainable params: 457,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Siamese Network With Two Identical Subnetworks\n",
    "left_input = keras.Input(shape = input_shape, name = 'left_input')\n",
    "right_input = keras.Input(shape = input_shape, name = 'right_input')\n",
    "\n",
    "subnetwork = create_subnetwork(input_shape, initializer)\n",
    "\n",
    "encoded_left = subnetwork(left_input) # chain layers\n",
    "encoded_right = subnetwork(right_input)\n",
    "\n",
    "# concatenate outputs of the two subnetworks\n",
    "concatted = keras.layers.Concatenate()([encoded_left, encoded_right])\n",
    "print(concatted.shape)\n",
    "\n",
    "comparatornetwork = create_comparatornetwork(concatted.shape[1], initializer)\n",
    "\n",
    "comparator = comparatornetwork(concatted)\n",
    "\n",
    "classificationUnit = keras.layers.Dense(name = 'classificationUnit', units = y.shape[1], kernel_initializer = initializer, activation = 'softmax')(comparator)\n",
    "\n",
    "leonet = keras.Model(name = 'LeONet', inputs = [left_input, right_input], outputs = classificationUnit)\n",
    "\n",
    "leonet.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "\n",
    "leonet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e44df40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"comparatornetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " comparatornetwork_input (In  [(None, 840)]            0         \n",
      " putLayer)                                                       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               168200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 194,625\n",
      "Trainable params: 194,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "comparatornetwork.summary()\n",
    "keras.utils.plot_model(comparatornetwork, show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a9059dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17598, 96)\n",
      "(17598, 6)\n",
      "(17598, 48)\n",
      "(17598, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# https://www.earthdatascience.org/courses/intro-to-earth-data-science/scientific-data-structures-python/numpy-arrays/indexing-slicing-numpy-arrays/\n",
    "\n",
    "x_train_left_input = x_train[:, 0:48] # select all rows and columns from 0th indexed to 47th indexed columns\n",
    "x_train_right_input = x_train[:, 48:96]\n",
    "\n",
    "# x_train_left_input = x_train[:, 0:28] # Existing 28 features\n",
    "# x_train_right_input = x_train[:, 48:76] # Existing 28 features\n",
    "\n",
    "# x_train_left_input = x_train[:, 28:48] # Novel 20 features\n",
    "# x_train_right_input = x_train[:, 76:96] # Novel 20 features\n",
    "\n",
    "print(x_train_left_input.shape)\n",
    "print(x_train_right_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af46f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "550/550 [==============================] - 45s 79ms/step - loss: 1.1439 - accuracy: 0.5285\n",
      "Epoch 2/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.5579 - accuracy: 0.7882\n",
      "Epoch 3/500\n",
      "550/550 [==============================] - 40s 72ms/step - loss: 0.3887 - accuracy: 0.8633\n",
      "Epoch 4/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.3228 - accuracy: 0.8875\n",
      "Epoch 5/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.2751 - accuracy: 0.9049\n",
      "Epoch 6/500\n",
      "550/550 [==============================] - 39s 70ms/step - loss: 0.2333 - accuracy: 0.9200\n",
      "Epoch 7/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.2054 - accuracy: 0.9325\n",
      "Epoch 8/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.1879 - accuracy: 0.9380\n",
      "Epoch 9/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.1655 - accuracy: 0.9474\n",
      "Epoch 10/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.1654 - accuracy: 0.9468\n",
      "Epoch 11/500\n",
      "550/550 [==============================] - 40s 72ms/step - loss: 0.1481 - accuracy: 0.9518\n",
      "Epoch 12/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.1446 - accuracy: 0.9520\n",
      "Epoch 13/500\n",
      "550/550 [==============================] - 40s 72ms/step - loss: 0.1343 - accuracy: 0.9556\n",
      "Epoch 14/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.1330 - accuracy: 0.9558\n",
      "Epoch 15/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.1180 - accuracy: 0.9615\n",
      "Epoch 16/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.1193 - accuracy: 0.9603\n",
      "Epoch 17/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.1050 - accuracy: 0.9637\n",
      "Epoch 18/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.0997 - accuracy: 0.9671\n",
      "Epoch 19/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0963 - accuracy: 0.9674\n",
      "Epoch 20/500\n",
      "550/550 [==============================] - 40s 72ms/step - loss: 0.1082 - accuracy: 0.9635\n",
      "Epoch 21/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.1039 - accuracy: 0.9664\n",
      "Epoch 22/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0955 - accuracy: 0.9685\n",
      "Epoch 23/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0893 - accuracy: 0.9706\n",
      "Epoch 24/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0788 - accuracy: 0.9744\n",
      "Epoch 25/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0860 - accuracy: 0.9721\n",
      "Epoch 26/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0921 - accuracy: 0.9708\n",
      "Epoch 27/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0742 - accuracy: 0.9748\n",
      "Epoch 28/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0762 - accuracy: 0.9751\n",
      "Epoch 29/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0755 - accuracy: 0.9748\n",
      "Epoch 30/500\n",
      "550/550 [==============================] - 40s 74ms/step - loss: 0.0880 - accuracy: 0.9720\n",
      "Epoch 31/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0685 - accuracy: 0.9777\n",
      "Epoch 32/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0654 - accuracy: 0.9763\n",
      "Epoch 33/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0649 - accuracy: 0.9776\n",
      "Epoch 34/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0895 - accuracy: 0.9722\n",
      "Epoch 35/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0743 - accuracy: 0.9757\n",
      "Epoch 36/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0657 - accuracy: 0.9773\n",
      "Epoch 37/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0698 - accuracy: 0.9781\n",
      "Epoch 38/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0653 - accuracy: 0.9774\n",
      "Epoch 39/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0606 - accuracy: 0.9801\n",
      "Epoch 40/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0549 - accuracy: 0.9807\n",
      "Epoch 41/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0585 - accuracy: 0.9798\n",
      "Epoch 42/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0594 - accuracy: 0.9789\n",
      "Epoch 43/500\n",
      "550/550 [==============================] - 42s 75ms/step - loss: 0.0553 - accuracy: 0.9810\n",
      "Epoch 44/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0547 - accuracy: 0.9824\n",
      "Epoch 45/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0756 - accuracy: 0.9772\n",
      "Epoch 46/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.0464 - accuracy: 0.9850\n",
      "Epoch 47/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0593 - accuracy: 0.9805\n",
      "Epoch 48/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0589 - accuracy: 0.9811\n",
      "Epoch 49/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0483 - accuracy: 0.9829\n",
      "Epoch 50/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0533 - accuracy: 0.9812\n",
      "Epoch 51/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0503 - accuracy: 0.9834\n",
      "Epoch 52/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0481 - accuracy: 0.9835\n",
      "Epoch 53/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0663 - accuracy: 0.9781\n",
      "Epoch 54/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0445 - accuracy: 0.9840\n",
      "Epoch 55/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.0462 - accuracy: 0.9841\n",
      "Epoch 56/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0584 - accuracy: 0.9810\n",
      "Epoch 57/500\n",
      "550/550 [==============================] - 40s 72ms/step - loss: 0.0419 - accuracy: 0.9852\n",
      "Epoch 58/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0407 - accuracy: 0.9860\n",
      "Epoch 59/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0465 - accuracy: 0.9851\n",
      "Epoch 60/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0456 - accuracy: 0.9851\n",
      "Epoch 61/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0382 - accuracy: 0.9866\n",
      "Epoch 62/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0412 - accuracy: 0.9866\n",
      "Epoch 63/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0646 - accuracy: 0.9803\n",
      "Epoch 64/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0421 - accuracy: 0.9860\n",
      "Epoch 65/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0359 - accuracy: 0.9882\n",
      "Epoch 66/500\n",
      "550/550 [==============================] - 42s 75ms/step - loss: 0.0367 - accuracy: 0.9866\n",
      "Epoch 67/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0384 - accuracy: 0.9864\n",
      "Epoch 68/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0417 - accuracy: 0.9859\n",
      "Epoch 69/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0488 - accuracy: 0.9845\n",
      "Epoch 70/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0380 - accuracy: 0.9868\n",
      "Epoch 71/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0419 - accuracy: 0.9844\n",
      "Epoch 72/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0508 - accuracy: 0.9830\n",
      "Epoch 73/500\n",
      "550/550 [==============================] - 40s 73ms/step - loss: 0.0353 - accuracy: 0.9881\n",
      "Epoch 74/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0493 - accuracy: 0.9869\n",
      "Epoch 75/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0493 - accuracy: 0.9849\n",
      "Epoch 76/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0380 - accuracy: 0.9869\n",
      "Epoch 77/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0364 - accuracy: 0.9872\n",
      "Epoch 78/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0424 - accuracy: 0.9860\n",
      "Epoch 79/500\n",
      "550/550 [==============================] - 39s 71ms/step - loss: 0.0327 - accuracy: 0.9877\n",
      "Epoch 80/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0438 - accuracy: 0.9851\n",
      "Epoch 81/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0331 - accuracy: 0.9882\n",
      "Epoch 82/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0338 - accuracy: 0.9880\n",
      "Epoch 83/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0302 - accuracy: 0.9887\n",
      "Epoch 84/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0820 - accuracy: 0.9786\n",
      "Epoch 85/500\n",
      "550/550 [==============================] - 41s 75ms/step - loss: 0.0471 - accuracy: 0.9851\n",
      "Epoch 86/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0303 - accuracy: 0.9895\n",
      "Epoch 87/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0321 - accuracy: 0.9887\n",
      "Epoch 88/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0300 - accuracy: 0.9883\n",
      "Epoch 89/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0325 - accuracy: 0.9881\n",
      "Epoch 90/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0535 - accuracy: 0.9839\n",
      "Epoch 91/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0314 - accuracy: 0.9890\n",
      "Epoch 92/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0297 - accuracy: 0.9891\n",
      "Epoch 93/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0336 - accuracy: 0.9885\n",
      "Epoch 94/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0324 - accuracy: 0.9885\n",
      "Epoch 95/500\n",
      "550/550 [==============================] - 43s 77ms/step - loss: 0.0389 - accuracy: 0.9863\n",
      "Epoch 96/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0373 - accuracy: 0.9862\n",
      "Epoch 97/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0343 - accuracy: 0.9883\n",
      "Epoch 98/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0280 - accuracy: 0.9897\n",
      "Epoch 99/500\n",
      "550/550 [==============================] - 43s 77ms/step - loss: 0.0462 - accuracy: 0.9858\n",
      "Epoch 100/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0371 - accuracy: 0.9880\n",
      "Epoch 101/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0542 - accuracy: 0.9848\n",
      "Epoch 102/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0445 - accuracy: 0.9868\n",
      "Epoch 103/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0484 - accuracy: 0.9870\n",
      "Epoch 104/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0285 - accuracy: 0.9890\n",
      "Epoch 105/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0338 - accuracy: 0.9885\n",
      "Epoch 106/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0406 - accuracy: 0.9857\n",
      "Epoch 107/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0366 - accuracy: 0.9870\n",
      "Epoch 108/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0283 - accuracy: 0.9894\n",
      "Epoch 109/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0247 - accuracy: 0.9909\n",
      "Epoch 110/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0281 - accuracy: 0.9896\n",
      "Epoch 111/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0363 - accuracy: 0.9877\n",
      "Epoch 112/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0281 - accuracy: 0.9900\n",
      "Epoch 113/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0446 - accuracy: 0.9860\n",
      "Epoch 114/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0295 - accuracy: 0.9894\n",
      "Epoch 115/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0312 - accuracy: 0.9884\n",
      "Epoch 116/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0298 - accuracy: 0.9899\n",
      "Epoch 117/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0324 - accuracy: 0.9894\n",
      "Epoch 118/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0310 - accuracy: 0.9889\n",
      "Epoch 119/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0368 - accuracy: 0.9882\n",
      "Epoch 120/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0286 - accuracy: 0.9901\n",
      "Epoch 121/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0321 - accuracy: 0.9887\n",
      "Epoch 122/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0408 - accuracy: 0.9877\n",
      "Epoch 123/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0353 - accuracy: 0.9883\n",
      "Epoch 124/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0437 - accuracy: 0.9862\n",
      "Epoch 125/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0315 - accuracy: 0.9898\n",
      "Epoch 126/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0253 - accuracy: 0.9909\n",
      "Epoch 127/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0254 - accuracy: 0.9901\n",
      "Epoch 128/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0233 - accuracy: 0.9907\n",
      "Epoch 129/500\n",
      "550/550 [==============================] - 41s 74ms/step - loss: 0.0246 - accuracy: 0.9908\n",
      "Epoch 130/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0394 - accuracy: 0.9884\n",
      "Epoch 131/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0339 - accuracy: 0.9886\n",
      "Epoch 132/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0324 - accuracy: 0.9889\n",
      "Epoch 133/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0408 - accuracy: 0.9878\n",
      "Epoch 134/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0293 - accuracy: 0.9907\n",
      "Epoch 135/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0409 - accuracy: 0.9876\n",
      "Epoch 136/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0253 - accuracy: 0.9919\n",
      "Epoch 137/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0248 - accuracy: 0.9904\n",
      "Epoch 138/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0276 - accuracy: 0.9903\n",
      "Epoch 139/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0307 - accuracy: 0.9895\n",
      "Epoch 140/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0310 - accuracy: 0.9889\n",
      "Epoch 141/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0290 - accuracy: 0.9897\n",
      "Epoch 142/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0284 - accuracy: 0.9907\n",
      "Epoch 143/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0239 - accuracy: 0.9912\n",
      "Epoch 144/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0243 - accuracy: 0.9907\n",
      "Epoch 145/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0296 - accuracy: 0.9895\n",
      "Epoch 146/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0338 - accuracy: 0.9893\n",
      "Epoch 147/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0363 - accuracy: 0.9890\n",
      "Epoch 148/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0401 - accuracy: 0.9880\n",
      "Epoch 149/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0261 - accuracy: 0.9908\n",
      "Epoch 150/500\n",
      "550/550 [==============================] - 42s 76ms/step - loss: 0.0368 - accuracy: 0.9886\n",
      "Epoch 151/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0298 - accuracy: 0.9892\n",
      "Epoch 152/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0640 - accuracy: 0.9864\n",
      "Epoch 153/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0307 - accuracy: 0.9898\n",
      "Epoch 154/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0546 - accuracy: 0.9837\n",
      "Epoch 155/500\n",
      "550/550 [==============================] - 43s 77ms/step - loss: 0.0268 - accuracy: 0.9902\n",
      "Epoch 156/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0257 - accuracy: 0.9900\n",
      "Epoch 157/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0240 - accuracy: 0.9914\n",
      "Epoch 158/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0217 - accuracy: 0.9919\n",
      "Epoch 159/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0310 - accuracy: 0.9890\n",
      "Epoch 160/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0231 - accuracy: 0.9906\n",
      "Epoch 161/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0263 - accuracy: 0.9916\n",
      "Epoch 162/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0287 - accuracy: 0.9905\n",
      "Epoch 163/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0245 - accuracy: 0.9911\n",
      "Epoch 164/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0197 - accuracy: 0.9927\n",
      "Epoch 165/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0366 - accuracy: 0.9884\n",
      "Epoch 166/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0303 - accuracy: 0.9899\n",
      "Epoch 167/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0252 - accuracy: 0.9910\n",
      "Epoch 168/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0230 - accuracy: 0.9908\n",
      "Epoch 169/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0319 - accuracy: 0.9894\n",
      "Epoch 170/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0307 - accuracy: 0.9893\n",
      "Epoch 171/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0302 - accuracy: 0.9893\n",
      "Epoch 172/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0210 - accuracy: 0.9927\n",
      "Epoch 173/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0303 - accuracy: 0.9899\n",
      "Epoch 174/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0272 - accuracy: 0.9907\n",
      "Epoch 175/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0299 - accuracy: 0.9896\n",
      "Epoch 176/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0271 - accuracy: 0.9901\n",
      "Epoch 177/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0250 - accuracy: 0.9907\n",
      "Epoch 178/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0281 - accuracy: 0.9906\n",
      "Epoch 179/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0325 - accuracy: 0.9895\n",
      "Epoch 180/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0233 - accuracy: 0.9914\n",
      "Epoch 181/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0303 - accuracy: 0.9897\n",
      "Epoch 182/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0200 - accuracy: 0.9920\n",
      "Epoch 183/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0235 - accuracy: 0.9916\n",
      "Epoch 184/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0263 - accuracy: 0.9910\n",
      "Epoch 185/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0363 - accuracy: 0.9893\n",
      "Epoch 186/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 187/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0265 - accuracy: 0.9914\n",
      "Epoch 188/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0299 - accuracy: 0.9903\n",
      "Epoch 189/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0405 - accuracy: 0.9889\n",
      "Epoch 190/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0310 - accuracy: 0.9902\n",
      "Epoch 191/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0349 - accuracy: 0.9895\n",
      "Epoch 192/500\n",
      "550/550 [==============================] - 43s 77ms/step - loss: 0.0223 - accuracy: 0.9916\n",
      "Epoch 193/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0248 - accuracy: 0.9912\n",
      "Epoch 194/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0425 - accuracy: 0.9880\n",
      "Epoch 195/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0338 - accuracy: 0.9891\n",
      "Epoch 196/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0245 - accuracy: 0.9910\n",
      "Epoch 197/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0559 - accuracy: 0.9873\n",
      "Epoch 198/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0389 - accuracy: 0.9884\n",
      "Epoch 199/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0233 - accuracy: 0.9911\n",
      "Epoch 200/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0199 - accuracy: 0.9927\n",
      "Epoch 201/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0195 - accuracy: 0.9924\n",
      "Epoch 202/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0572 - accuracy: 0.9864\n",
      "Epoch 203/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0296 - accuracy: 0.9900\n",
      "Epoch 204/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0243 - accuracy: 0.9902\n",
      "Epoch 205/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0200 - accuracy: 0.9926\n",
      "Epoch 206/500\n",
      "550/550 [==============================] - 42s 77ms/step - loss: 0.0236 - accuracy: 0.9909\n",
      "Epoch 207/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0234 - accuracy: 0.9922\n",
      "Epoch 208/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0241 - accuracy: 0.9923\n",
      "Epoch 209/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0336 - accuracy: 0.9902\n",
      "Epoch 210/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0325 - accuracy: 0.9887\n",
      "Epoch 211/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0211 - accuracy: 0.9917\n",
      "Epoch 212/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0231 - accuracy: 0.9917\n",
      "Epoch 213/500\n",
      "550/550 [==============================] - 45s 83ms/step - loss: 0.0311 - accuracy: 0.9902\n",
      "Epoch 214/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0250 - accuracy: 0.9912\n",
      "Epoch 215/500\n",
      "550/550 [==============================] - 43s 78ms/step - loss: 0.0205 - accuracy: 0.9924\n",
      "Epoch 216/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0199 - accuracy: 0.9926\n",
      "Epoch 217/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0210 - accuracy: 0.9918\n",
      "Epoch 218/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0233 - accuracy: 0.9912\n",
      "Epoch 219/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0548 - accuracy: 0.9852\n",
      "Epoch 220/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0674 - accuracy: 0.9841\n",
      "Epoch 221/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0545 - accuracy: 0.9878\n",
      "Epoch 222/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0251 - accuracy: 0.9903\n",
      "Epoch 223/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0181 - accuracy: 0.9931\n",
      "Epoch 224/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0238 - accuracy: 0.9916\n",
      "Epoch 225/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0192 - accuracy: 0.9930\n",
      "Epoch 226/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0387 - accuracy: 0.9902\n",
      "Epoch 227/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0290 - accuracy: 0.9915\n",
      "Epoch 228/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0409 - accuracy: 0.9876\n",
      "Epoch 229/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0230 - accuracy: 0.9912\n",
      "Epoch 230/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0202 - accuracy: 0.9917\n",
      "Epoch 231/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0228 - accuracy: 0.9918\n",
      "Epoch 232/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0258 - accuracy: 0.9906\n",
      "Epoch 233/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0382 - accuracy: 0.9894\n",
      "Epoch 234/500\n",
      "550/550 [==============================] - 44s 79ms/step - loss: 0.0203 - accuracy: 0.9925\n",
      "Epoch 235/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0217 - accuracy: 0.9919\n",
      "Epoch 236/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0263 - accuracy: 0.9907\n",
      "Epoch 237/500\n",
      "550/550 [==============================] - 43s 79ms/step - loss: 0.0244 - accuracy: 0.9901\n",
      "Epoch 238/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0432 - accuracy: 0.9868\n",
      "Epoch 239/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0415 - accuracy: 0.9884\n",
      "Epoch 240/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0232 - accuracy: 0.9915\n",
      "Epoch 241/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0228 - accuracy: 0.9916\n",
      "Epoch 242/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0203 - accuracy: 0.9925\n",
      "Epoch 243/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0297 - accuracy: 0.9905\n",
      "Epoch 244/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0178 - accuracy: 0.9932\n",
      "Epoch 245/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0248 - accuracy: 0.9910\n",
      "Epoch 246/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0332 - accuracy: 0.9895\n",
      "Epoch 247/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0769 - accuracy: 0.9899\n",
      "Epoch 248/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0319 - accuracy: 0.9901\n",
      "Epoch 249/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0306 - accuracy: 0.9906\n",
      "Epoch 250/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0187 - accuracy: 0.9929\n",
      "Epoch 251/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0769 - accuracy: 0.9893\n",
      "Epoch 252/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0278 - accuracy: 0.9899\n",
      "Epoch 253/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0200 - accuracy: 0.9922\n",
      "Epoch 254/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0224 - accuracy: 0.9927\n",
      "Epoch 255/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0155 - accuracy: 0.9936\n",
      "Epoch 256/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0207 - accuracy: 0.9924\n",
      "Epoch 257/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0582 - accuracy: 0.9884\n",
      "Epoch 258/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0372 - accuracy: 0.9877\n",
      "Epoch 259/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0197 - accuracy: 0.9924\n",
      "Epoch 260/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0180 - accuracy: 0.9927\n",
      "Epoch 261/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0198 - accuracy: 0.9926\n",
      "Epoch 262/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0248 - accuracy: 0.9914\n",
      "Epoch 263/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0218 - accuracy: 0.9920\n",
      "Epoch 264/500\n",
      "550/550 [==============================] - 44s 80ms/step - loss: 0.0182 - accuracy: 0.9934\n",
      "Epoch 265/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0247 - accuracy: 0.9910\n",
      "Epoch 266/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0242 - accuracy: 0.9914\n",
      "Epoch 267/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0353 - accuracy: 0.9896\n",
      "Epoch 268/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0234 - accuracy: 0.9912\n",
      "Epoch 269/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0299 - accuracy: 0.9902\n",
      "Epoch 270/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0311 - accuracy: 0.9906\n",
      "Epoch 271/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0297 - accuracy: 0.9912\n",
      "Epoch 272/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0199 - accuracy: 0.9922\n",
      "Epoch 273/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0651 - accuracy: 0.9855\n",
      "Epoch 274/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0395 - accuracy: 0.9887\n",
      "Epoch 275/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0228 - accuracy: 0.9916\n",
      "Epoch 276/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0508 - accuracy: 0.9901\n",
      "Epoch 277/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0564 - accuracy: 0.9915\n",
      "Epoch 278/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0314 - accuracy: 0.9903\n",
      "Epoch 279/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0197 - accuracy: 0.9925\n",
      "Epoch 280/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0184 - accuracy: 0.9933\n",
      "Epoch 281/500\n",
      "550/550 [==============================] - 44s 81ms/step - loss: 0.0242 - accuracy: 0.9922\n",
      "Epoch 282/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0195 - accuracy: 0.9931\n",
      "Epoch 283/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0263 - accuracy: 0.9912\n",
      "Epoch 284/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0281 - accuracy: 0.9911\n",
      "Epoch 285/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0339 - accuracy: 0.9916\n",
      "Epoch 286/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0291 - accuracy: 0.9912\n",
      "Epoch 287/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0337 - accuracy: 0.9910\n",
      "Epoch 288/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0247 - accuracy: 0.9912\n",
      "Epoch 289/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0247 - accuracy: 0.9906\n",
      "Epoch 290/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0296 - accuracy: 0.9905\n",
      "Epoch 291/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0251 - accuracy: 0.9912\n",
      "Epoch 292/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0420 - accuracy: 0.9881\n",
      "Epoch 293/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0255 - accuracy: 0.9914\n",
      "Epoch 294/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.1038 - accuracy: 0.9845\n",
      "Epoch 295/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0261 - accuracy: 0.9913\n",
      "Epoch 296/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0191 - accuracy: 0.9932\n",
      "Epoch 297/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0200 - accuracy: 0.9927\n",
      "Epoch 298/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0213 - accuracy: 0.9924\n",
      "Epoch 299/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0232 - accuracy: 0.9923\n",
      "Epoch 300/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0187 - accuracy: 0.9934\n",
      "Epoch 301/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0178 - accuracy: 0.9926\n",
      "Epoch 302/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0314 - accuracy: 0.9902\n",
      "Epoch 303/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0499 - accuracy: 0.9899\n",
      "Epoch 304/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0228 - accuracy: 0.9916\n",
      "Epoch 305/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0252 - accuracy: 0.9914\n",
      "Epoch 306/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0238 - accuracy: 0.9913\n",
      "Epoch 307/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0174 - accuracy: 0.9931\n",
      "Epoch 308/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0164 - accuracy: 0.9932\n",
      "Epoch 309/500\n",
      "550/550 [==============================] - 50s 92ms/step - loss: 0.0418 - accuracy: 0.9872\n",
      "Epoch 310/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0245 - accuracy: 0.9916\n",
      "Epoch 311/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0313 - accuracy: 0.9904\n",
      "Epoch 312/500\n",
      "550/550 [==============================] - 45s 83ms/step - loss: 0.0219 - accuracy: 0.9918\n",
      "Epoch 313/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0286 - accuracy: 0.9910\n",
      "Epoch 314/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0241 - accuracy: 0.9918\n",
      "Epoch 315/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0276 - accuracy: 0.9918\n",
      "Epoch 316/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0468 - accuracy: 0.9878\n",
      "Epoch 317/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0900 - accuracy: 0.9856\n",
      "Epoch 318/500\n",
      "550/550 [==============================] - 45s 82ms/step - loss: 0.0313 - accuracy: 0.9892\n",
      "Epoch 319/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0327 - accuracy: 0.9904\n",
      "Epoch 320/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0556 - accuracy: 0.9875\n",
      "Epoch 321/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0253 - accuracy: 0.9922\n",
      "Epoch 322/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0202 - accuracy: 0.9926\n",
      "Epoch 323/500\n",
      "550/550 [==============================] - 51s 92ms/step - loss: 0.0239 - accuracy: 0.9919\n",
      "Epoch 324/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0363 - accuracy: 0.9918\n",
      "Epoch 325/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0186 - accuracy: 0.9928\n",
      "Epoch 326/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0205 - accuracy: 0.9925\n",
      "Epoch 327/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0382 - accuracy: 0.9891\n",
      "Epoch 328/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0770 - accuracy: 0.9864\n",
      "Epoch 329/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0294 - accuracy: 0.9905\n",
      "Epoch 330/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0207 - accuracy: 0.9920\n",
      "Epoch 331/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0322 - accuracy: 0.9899\n",
      "Epoch 332/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0191 - accuracy: 0.9926\n",
      "Epoch 333/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0210 - accuracy: 0.9923\n",
      "Epoch 334/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0187 - accuracy: 0.9929\n",
      "Epoch 335/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0222 - accuracy: 0.9924\n",
      "Epoch 336/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0263 - accuracy: 0.9922\n",
      "Epoch 337/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0162 - accuracy: 0.9937\n",
      "Epoch 338/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0307 - accuracy: 0.9909\n",
      "Epoch 339/500\n",
      "550/550 [==============================] - 45s 81ms/step - loss: 0.0238 - accuracy: 0.9914\n",
      "Epoch 340/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0245 - accuracy: 0.9920\n",
      "Epoch 341/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0359 - accuracy: 0.9912\n",
      "Epoch 342/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0395 - accuracy: 0.9897\n",
      "Epoch 343/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0263 - accuracy: 0.9926\n",
      "Epoch 344/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0353 - accuracy: 0.9909\n",
      "Epoch 345/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0417 - accuracy: 0.9895\n",
      "Epoch 346/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0732 - accuracy: 0.9881\n",
      "Epoch 347/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0231 - accuracy: 0.9922\n",
      "Epoch 348/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0230 - accuracy: 0.9912\n",
      "Epoch 349/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0165 - accuracy: 0.9932\n",
      "Epoch 350/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0156 - accuracy: 0.9941\n",
      "Epoch 351/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0178 - accuracy: 0.9929\n",
      "Epoch 352/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0300 - accuracy: 0.9910\n",
      "Epoch 353/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0426 - accuracy: 0.9890\n",
      "Epoch 354/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0221 - accuracy: 0.9923\n",
      "Epoch 355/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0210 - accuracy: 0.9924\n",
      "Epoch 356/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0296 - accuracy: 0.9914\n",
      "Epoch 357/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0394 - accuracy: 0.9896\n",
      "Epoch 358/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0285 - accuracy: 0.9907\n",
      "Epoch 359/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0199 - accuracy: 0.9927\n",
      "Epoch 360/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0273 - accuracy: 0.9911\n",
      "Epoch 361/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0360 - accuracy: 0.9903\n",
      "Epoch 362/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0212 - accuracy: 0.9926\n",
      "Epoch 363/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0192 - accuracy: 0.9926\n",
      "Epoch 364/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0210 - accuracy: 0.9923\n",
      "Epoch 365/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0253 - accuracy: 0.9916\n",
      "Epoch 366/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0243 - accuracy: 0.9910\n",
      "Epoch 367/500\n",
      "550/550 [==============================] - 47s 85ms/step - loss: 0.0346 - accuracy: 0.9898\n",
      "Epoch 368/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0276 - accuracy: 0.9899\n",
      "Epoch 369/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0643 - accuracy: 0.9872\n",
      "Epoch 370/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0426 - accuracy: 0.9880\n",
      "Epoch 371/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 372/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0195 - accuracy: 0.9926\n",
      "Epoch 373/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0205 - accuracy: 0.9927\n",
      "Epoch 374/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0271 - accuracy: 0.9912\n",
      "Epoch 375/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0180 - accuracy: 0.9936\n",
      "Epoch 376/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.1685 - accuracy: 0.9884\n",
      "Epoch 377/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0417 - accuracy: 0.9880\n",
      "Epoch 378/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0273 - accuracy: 0.9911\n",
      "Epoch 379/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0256 - accuracy: 0.9911\n",
      "Epoch 380/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0172 - accuracy: 0.9935\n",
      "Epoch 381/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0244 - accuracy: 0.9935\n",
      "Epoch 382/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0243 - accuracy: 0.9920\n",
      "Epoch 383/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0204 - accuracy: 0.9923\n",
      "Epoch 384/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0179 - accuracy: 0.9935\n",
      "Epoch 385/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0185 - accuracy: 0.9927\n",
      "Epoch 386/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0533 - accuracy: 0.9857\n",
      "Epoch 387/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0296 - accuracy: 0.9903\n",
      "Epoch 388/500\n",
      "550/550 [==============================] - 52s 95ms/step - loss: 0.0208 - accuracy: 0.9924\n",
      "Epoch 389/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0171 - accuracy: 0.9932\n",
      "Epoch 390/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 391/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0391 - accuracy: 0.9906\n",
      "Epoch 392/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0233 - accuracy: 0.9918\n",
      "Epoch 393/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0203 - accuracy: 0.9927\n",
      "Epoch 394/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0239 - accuracy: 0.9917\n",
      "Epoch 395/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0199 - accuracy: 0.9929\n",
      "Epoch 396/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0392 - accuracy: 0.9889\n",
      "Epoch 397/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0214 - accuracy: 0.9926\n",
      "Epoch 398/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0160 - accuracy: 0.9936\n",
      "Epoch 399/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0167 - accuracy: 0.9930\n",
      "Epoch 400/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0199 - accuracy: 0.9923\n",
      "Epoch 401/500\n",
      "550/550 [==============================] - 46s 84ms/step - loss: 0.0348 - accuracy: 0.9909\n",
      "Epoch 402/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0302 - accuracy: 0.9906\n",
      "Epoch 403/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0242 - accuracy: 0.9922\n",
      "Epoch 404/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0289 - accuracy: 0.9906\n",
      "Epoch 405/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0821 - accuracy: 0.9882\n",
      "Epoch 406/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0348 - accuracy: 0.9901\n",
      "Epoch 407/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0337 - accuracy: 0.9914\n",
      "Epoch 408/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0206 - accuracy: 0.9928\n",
      "Epoch 409/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0236 - accuracy: 0.9926\n",
      "Epoch 410/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0250 - accuracy: 0.9918\n",
      "Epoch 411/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0193 - accuracy: 0.9930\n",
      "Epoch 412/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0209 - accuracy: 0.9928\n",
      "Epoch 413/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0464 - accuracy: 0.9894\n",
      "Epoch 414/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0271 - accuracy: 0.9913\n",
      "Epoch 415/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0213 - accuracy: 0.9923\n",
      "Epoch 416/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0200 - accuracy: 0.9931\n",
      "Epoch 417/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0446 - accuracy: 0.9895\n",
      "Epoch 418/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0374 - accuracy: 0.9897\n",
      "Epoch 419/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0300 - accuracy: 0.9904\n",
      "Epoch 420/500\n",
      "550/550 [==============================] - 51s 93ms/step - loss: 0.0168 - accuracy: 0.9938\n",
      "Epoch 421/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0201 - accuracy: 0.9922\n",
      "Epoch 422/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0196 - accuracy: 0.9925\n",
      "Epoch 423/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0250 - accuracy: 0.9913\n",
      "Epoch 424/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0525 - accuracy: 0.9934\n",
      "Epoch 425/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0245 - accuracy: 0.9913\n",
      "Epoch 426/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0185 - accuracy: 0.9927\n",
      "Epoch 427/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0223 - accuracy: 0.9922\n",
      "Epoch 428/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0297 - accuracy: 0.9910\n",
      "Epoch 429/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0453 - accuracy: 0.9884\n",
      "Epoch 430/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0193 - accuracy: 0.9934\n",
      "Epoch 431/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0179 - accuracy: 0.9928\n",
      "Epoch 432/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0171 - accuracy: 0.9932\n",
      "Epoch 433/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0513 - accuracy: 0.9870\n",
      "Epoch 434/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0334 - accuracy: 0.9908\n",
      "Epoch 435/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0280 - accuracy: 0.9906\n",
      "Epoch 436/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0382 - accuracy: 0.9916\n",
      "Epoch 437/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0187 - accuracy: 0.9928\n",
      "Epoch 438/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0153 - accuracy: 0.9938\n",
      "Epoch 439/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0178 - accuracy: 0.9929\n",
      "Epoch 440/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0312 - accuracy: 0.9905\n",
      "Epoch 441/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0191 - accuracy: 0.9927\n",
      "Epoch 442/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.1447 - accuracy: 0.9826\n",
      "Epoch 443/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0404 - accuracy: 0.9866\n",
      "Epoch 444/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0291 - accuracy: 0.9902\n",
      "Epoch 445/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0232 - accuracy: 0.9918\n",
      "Epoch 446/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0166 - accuracy: 0.9937\n",
      "Epoch 447/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0339 - accuracy: 0.9922\n",
      "Epoch 448/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0311 - accuracy: 0.9903\n",
      "Epoch 449/500\n",
      "550/550 [==============================] - 51s 92ms/step - loss: 0.0187 - accuracy: 0.9935\n",
      "Epoch 450/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0542 - accuracy: 0.9896\n",
      "Epoch 451/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0184 - accuracy: 0.9931\n",
      "Epoch 452/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0201 - accuracy: 0.9928\n",
      "Epoch 453/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0170 - accuracy: 0.9935\n",
      "Epoch 454/500\n",
      "550/550 [==============================] - 48s 86ms/step - loss: 0.0170 - accuracy: 0.9933\n",
      "Epoch 455/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0849 - accuracy: 0.9879\n",
      "Epoch 456/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0392 - accuracy: 0.9895\n",
      "Epoch 457/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0204 - accuracy: 0.9929\n",
      "Epoch 458/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0174 - accuracy: 0.9933\n",
      "Epoch 459/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0185 - accuracy: 0.9934\n",
      "Epoch 460/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0175 - accuracy: 0.9933\n",
      "Epoch 461/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0453 - accuracy: 0.9884\n",
      "Epoch 462/500\n",
      "550/550 [==============================] - 49s 88ms/step - loss: 0.0611 - accuracy: 0.9853\n",
      "Epoch 463/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0295 - accuracy: 0.9909\n",
      "Epoch 464/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0229 - accuracy: 0.9922\n",
      "Epoch 465/500\n",
      "550/550 [==============================] - 51s 93ms/step - loss: 0.0291 - accuracy: 0.9905\n",
      "Epoch 466/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0197 - accuracy: 0.9930\n",
      "Epoch 467/500\n",
      "550/550 [==============================] - 46s 83ms/step - loss: 0.0509 - accuracy: 0.9879\n",
      "Epoch 468/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0308 - accuracy: 0.9899\n",
      "Epoch 469/500\n",
      "550/550 [==============================] - 50s 92ms/step - loss: 0.0178 - accuracy: 0.9930\n",
      "Epoch 470/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0175 - accuracy: 0.9934\n",
      "Epoch 471/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0161 - accuracy: 0.9936\n",
      "Epoch 472/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0262 - accuracy: 0.9914\n",
      "Epoch 473/500\n",
      "550/550 [==============================] - 47s 86ms/step - loss: 0.0253 - accuracy: 0.9919\n",
      "Epoch 474/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0897 - accuracy: 0.9815\n",
      "Epoch 475/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0395 - accuracy: 0.9880\n",
      "Epoch 476/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0335 - accuracy: 0.9890\n",
      "Epoch 477/500\n",
      "550/550 [==============================] - 54s 98ms/step - loss: 0.0280 - accuracy: 0.9915\n",
      "Epoch 478/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0226 - accuracy: 0.9920\n",
      "Epoch 479/500\n",
      "550/550 [==============================] - 51s 92ms/step - loss: 0.0178 - accuracy: 0.9933\n",
      "Epoch 480/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0267 - accuracy: 0.9924\n",
      "Epoch 481/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0248 - accuracy: 0.9911\n",
      "Epoch 482/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0243 - accuracy: 0.9916\n",
      "Epoch 483/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 484/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.1583 - accuracy: 0.9800\n",
      "Epoch 485/500\n",
      "550/550 [==============================] - 49s 90ms/step - loss: 0.0329 - accuracy: 0.9905\n",
      "Epoch 486/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0233 - accuracy: 0.9919\n",
      "Epoch 487/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0223 - accuracy: 0.9920\n",
      "Epoch 488/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0174 - accuracy: 0.9932\n",
      "Epoch 489/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0203 - accuracy: 0.9926\n",
      "Epoch 490/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0372 - accuracy: 0.9912\n",
      "Epoch 491/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0249 - accuracy: 0.9922\n",
      "Epoch 492/500\n",
      "550/550 [==============================] - 50s 90ms/step - loss: 0.0213 - accuracy: 0.9926\n",
      "Epoch 493/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0281 - accuracy: 0.9908\n",
      "Epoch 494/500\n",
      "550/550 [==============================] - 48s 87ms/step - loss: 0.0173 - accuracy: 0.9931\n",
      "Epoch 495/500\n",
      "550/550 [==============================] - 49s 89ms/step - loss: 0.0271 - accuracy: 0.9920\n",
      "Epoch 496/500\n",
      "550/550 [==============================] - 48s 88ms/step - loss: 0.0259 - accuracy: 0.9921\n",
      "Epoch 497/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0272 - accuracy: 0.9922\n",
      "Epoch 498/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0658 - accuracy: 0.9856\n",
      "Epoch 499/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0271 - accuracy: 0.9912\n",
      "Epoch 500/500\n",
      "550/550 [==============================] - 50s 91ms/step - loss: 0.0211 - accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# start timer\n",
    "start_time = time.time()\n",
    "\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('leonet_epoch_500_training.log', separator=',', append=False)\n",
    "\n",
    "# Fit the ANN to the Training set\n",
    "historyObject = leonet.fit(x = [x_train_left_input, x_train_right_input], y = y_train, batch_size = 32, epochs = 500, callbacks=[csv_logger])\n",
    "\n",
    "# Stop timer clock\n",
    "elapsed = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2418f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22874.31223464012\n",
      "Training duration: 6:21:14.312235\n"
     ]
    }
   ],
   "source": [
    "print(elapsed)\n",
    "\n",
    "# Calculate K-fold execution duration\n",
    "print(\"Training duration: \" + str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006743d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('models/leonet_epoch_500.h5') is False:\n",
    "    leonet.save('models/leonet_epoch_500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45ac361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7542, 96)\n",
      "(7542, 6)\n",
      "(7542, 48)\n",
      "(7542, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# https://www.earthdatascience.org/courses/intro-to-earth-data-science/scientific-data-structures-python/numpy-arrays/indexing-slicing-numpy-arrays/\n",
    "\n",
    "x_test_left_input = x_test[:, 0:48] # select all rows and columns from 0th indexed to 47th indexed columns\n",
    "x_test_right_input = x_test[:, 48:96]\n",
    "\n",
    "# x_test_left_input = x_test[:, 0:28] # Existing 28 features\n",
    "# x_test_right_input = x_test[:, 48:76] # Existing 28 features\n",
    "\n",
    "# x_test_left_input = x_test[:, 28:48] # Novel 20 features\n",
    "# x_test_right_input = x_test[:, 76:96] # Novel 20 features\n",
    "\n",
    "print(x_test_left_input.shape)\n",
    "print(x_test_right_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4430ea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 2s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# 4. Predict the Test Set Results\n",
    "import numpy as np\n",
    "y_pred = leonet.predict(x = [x_test_left_input, x_test_right_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd510f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7542, 6)\n",
      "[3 5 4 ... 2 3 0]\n",
      "[[0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0]\n",
      " ...\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0]]\n",
      "(7542, 6)\n",
      "[3 5 4 ... 2 3 0]\n",
      "(7542,)\n"
     ]
    }
   ],
   "source": [
    "# 5. Converting y_test, and y_pred from One-Hot encoding to integer encoding.\n",
    "\n",
    "# https://stackoverflow.com/questions/47564495/what-does-numpy-ndarray-shape-do\n",
    "# For a 1D array, the shape would be (n,) where n is the number of elements in your array.\n",
    "# For a 2D array, the shape would be (n,m) where n is the number of rows and m is the number of columns in your array.\n",
    "\n",
    "# Converting y_pred from One-Hot encoding to integer encoding.\n",
    "# y_pred = <class 'numpy.ndarray'>\n",
    "\n",
    "# shape is a tuple that always gives dimensions of the array.\n",
    "print(y_pred.shape) # (7542, 6) means 7542 rows and 6 columns\n",
    "y_pred = np.argmax(y_pred, axis = -1) # (axis = -1) represents the last axis. In this case, 6.\n",
    "print(y_pred)\n",
    "\n",
    "# collect y_test predicted class values\n",
    "# https://stackoverflow.com/questions/47435526/what-is-the-meaning-of-axis-1-in-keras-argmax\n",
    "print(y_test)\n",
    "print(y_test.shape) # (7542, 6) means 7542 rows and 6 columns\n",
    "y_test = np.argmax(y_test, axis = -1) # Returns the indices of the maximum values along an axis. (axis = -1) represents the last axis. In this case, values from 0 to 5 (predicted target classes).\n",
    "print(y_test)\n",
    "print(y_test.shape) # (7542,) means 1D array where 7542 elements available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d60b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Type 1       1.00      0.97      0.98      1273\n",
      "        VST3       0.94      0.96      0.95      1267\n",
      "         ST3       0.95      0.95      0.95      1262\n",
      "         MT3       0.98      0.99      0.98      1261\n",
      "       WT3/4       1.00      1.00      1.00      1243\n",
      "       False       1.00      1.00      1.00      1236\n",
      "\n",
      "    accuracy                           0.98      7542\n",
      "   macro avg       0.98      0.98      0.98      7542\n",
      "weighted avg       0.98      0.98      0.98      7542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
    "\n",
    "target_names = ['Type 1', 'VST3', 'ST3', 'MT3', 'WT3/4', 'False']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# macro average (averaging the unweighted mean per label)\n",
    "# weighted average (averaging the support-weighted mean per label)\n",
    "# Support is the number of actual occurrences of the class in the specified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b0bc20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy score is: 97.83%\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final accuracy score is: {:0.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59a98c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weighted precision score is: 97.85%\n"
     ]
    }
   ],
   "source": [
    "# precision weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "precisionweighted = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted precision score is: {:0.2f}%\".format(precisionweighted * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3e62ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weighted recall score is: 97.83%\n"
     ]
    }
   ],
   "source": [
    "# recall weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "recallweighted = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted recall score is: {:0.2f}%\".format(recallweighted * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebb39fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weighted f1 score is: 97.83%\n"
     ]
    }
   ],
   "source": [
    "# f1 weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "f1weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted f1 score is: {:0.2f}%\".format(f1weighted * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93807b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_recall = []\n",
    "vst3_recall = []\n",
    "st3_recall = []\n",
    "mt3_recall = []\n",
    "wt3_recall = []\n",
    "t0_recall = []\n",
    "\n",
    "def appendRecallForEachClass(arr):\n",
    "    t1_recall.append(arr[0])\n",
    "    vst3_recall.append(arr[1])\n",
    "    st3_recall.append(arr[2])\n",
    "    mt3_recall.append(arr[3])\n",
    "    wt3_recall.append(arr[4])\n",
    "    t0_recall.append(arr[5])\n",
    "    # printRecallForEachClass(arr)\n",
    "\n",
    "def printRecallForEachClass(arr):\n",
    "    print(arr)\n",
    "    print(t1_recall)\n",
    "    print(vst3_recall)\n",
    "    print(st3_recall)\n",
    "    print(mt3_recall)\n",
    "    print(wt3_recall)\n",
    "    print(t0_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0f9b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_precision = []\n",
    "vst3_precision = []\n",
    "st3_precision = []\n",
    "mt3_precision = []\n",
    "wt3_precision = []\n",
    "t0_precision = []\n",
    "\n",
    "def appendPrecisionForEachClass(arr):\n",
    "    t1_precision.append(arr[0])\n",
    "    vst3_precision.append(arr[1])\n",
    "    st3_precision.append(arr[2])\n",
    "    mt3_precision.append(arr[3])\n",
    "    wt3_precision.append(arr[4])\n",
    "    t0_precision.append(arr[5])\n",
    "    # printPrecisionForEachClass(arr)\n",
    "\n",
    "def printPrecisionForEachClass(arr):\n",
    "    print(arr)\n",
    "    print(t1_precision)\n",
    "    print(vst3_precision)\n",
    "    print(st3_precision)\n",
    "    print(mt3_precision)\n",
    "    print(wt3_precision)\n",
    "    print(t0_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2b43378",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_f1score = []\n",
    "vst3_f1score = []\n",
    "st3_f1score = []\n",
    "mt3_f1score = []\n",
    "wt3_f1score = []\n",
    "t0_f1score = []\n",
    "\n",
    "def appendF1ScoreForEachClass(arr):\n",
    "    t1_f1score.append(arr[0])\n",
    "    vst3_f1score.append(arr[1])\n",
    "    st3_f1score.append(arr[2])\n",
    "    mt3_f1score.append(arr[3])\n",
    "    wt3_f1score.append(arr[4])\n",
    "    t0_f1score.append(arr[5])\n",
    "    # printF1ScoreForEachClass(arr)\n",
    "\n",
    "def printF1ScoreForEachClass(arr):\n",
    "    print(arr)\n",
    "    print(t1_f1score)\n",
    "    print(vst3_f1score)\n",
    "    print(st3_f1score)\n",
    "    print(mt3_f1score)\n",
    "    print(wt3_f1score)\n",
    "    print(t0_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5a1373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro recall score is: 97.84%\n",
      "Final micro recall score is: 97.83%\n",
      "Final weighted recall score is: 97.83%\n",
      "Final macro precision score is: 97.86%\n",
      "Final micro precision score is: 97.83%\n",
      "Final weighted precision score is: 97.85%\n",
      "Final macro f1 score is: 97.84%\n",
      "Final micro f1 score is: 97.83%\n",
      "Final weighted f1 score is: 97.83%\n"
     ]
    }
   ],
   "source": [
    "# recall\n",
    "\n",
    "# recall macro average - This parameter is required for multiclass/multilabel targets.\n",
    "recallmacro = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Final macro recall score is: {:0.2f}%\".format(recallmacro * 100))\n",
    "\n",
    "# recall micro average - This parameter is required for multiclass/multilabel targets.\n",
    "recallmicro = recall_score(y_test, y_pred, average='micro')\n",
    "print(\"Final micro recall score is: {:0.2f}%\".format(recallmicro * 100))\n",
    "\n",
    "# recall weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "recallweighted = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted recall score is: {:0.2f}%\".format(recallweighted * 100))\n",
    "\n",
    "# precision\n",
    "\n",
    "# precision macro average - This parameter is required for multiclass/multilabel targets.\n",
    "precisionmacro = precision_score(y_test, y_pred, average='macro')\n",
    "print(\"Final macro precision score is: {:0.2f}%\".format(precisionmacro * 100))\n",
    "\n",
    "# precision micro average - This parameter is required for multiclass/multilabel targets.\n",
    "precisionmicro = precision_score(y_test, y_pred, average='micro')\n",
    "print(\"Final micro precision score is: {:0.2f}%\".format(precisionmicro * 100))\n",
    "\n",
    "# precision weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "precisionweighted = precision_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted precision score is: {:0.2f}%\".format(precisionweighted * 100))\n",
    "\n",
    "# f1 score\n",
    "\n",
    "# f1 macro average - This parameter is required for multiclass/multilabel targets.\n",
    "f1macro = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"Final macro f1 score is: {:0.2f}%\".format(f1macro * 100))\n",
    "\n",
    "# f1 micro average - This parameter is required for multiclass/multilabel targets.\n",
    "f1micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"Final micro f1 score is: {:0.2f}%\".format(f1micro * 100))\n",
    "\n",
    "# f1 weighted average - This parameter is required for multiclass/multilabel targets.\n",
    "f1weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Final weighted f1 score is: {:0.2f}%\".format(f1weighted * 100))\n",
    "\n",
    "# collect details class-vice\n",
    "appendRecallForEachClass(recall_score(y_test, y_pred, average=None))\n",
    "appendPrecisionForEachClass(precision_score(y_test, y_pred, average=None))\n",
    "appendF1ScoreForEachClass(f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40290785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Type 1 Recall: 96.78%\n",
      "VST3 Recall: 95.90%\n",
      "ST3 Recall: 95.40%\n",
      "MT3 Recall: 98.97%\n",
      "WT3/4 Recall: 100.00%\n",
      "False Recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Recall by Class\n",
    "print(len(t1_recall))\n",
    "print(\"Type 1 Recall: {:0.2f}%\".format(mean(t1_recall) * 100))\n",
    "print(\"VST3 Recall: {:0.2f}%\".format(mean(vst3_recall) * 100))\n",
    "print(\"ST3 Recall: {:0.2f}%\".format(mean(st3_recall) * 100))\n",
    "print(\"MT3 Recall: {:0.2f}%\".format(mean(mt3_recall) * 100))\n",
    "print(\"WT3/4 Recall: {:0.2f}%\".format(mean(wt3_recall) * 100))\n",
    "print(\"False Recall: {:0.2f}%\".format(mean(t0_recall) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77364ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Type 1 Precision: 99.92%\n",
      "VST3 Precision: 94.11%\n",
      "ST3 Precision: 95.40%\n",
      "MT3 Precision: 97.73%\n",
      "WT3/4 Precision: 100.00%\n",
      "False Precision: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Precision by Class\n",
    "print(len(wt3_precision))\n",
    "print(\"Type 1 Precision: {:0.2f}%\".format(mean(t1_precision) * 100))\n",
    "print(\"VST3 Precision: {:0.2f}%\".format(mean(vst3_precision) * 100))\n",
    "print(\"ST3 Precision: {:0.2f}%\".format(mean(st3_precision) * 100))\n",
    "print(\"MT3 Precision: {:0.2f}%\".format(mean(mt3_precision) * 100))\n",
    "print(\"WT3/4 Precision: {:0.2f}%\".format(mean(wt3_precision) * 100))\n",
    "print(\"False Precision: {:0.2f}%\".format(mean(t0_precision) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b9dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Type 1 F1 Score: 98.32%\n",
      "VST3 F1 Score: 95.00%\n",
      "ST3 F1 Score: 95.40%\n",
      "MT3 F1 Score: 98.35%\n",
      "WT3/4 F1 Score: 100.00%\n",
      "False F1 Score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# F1 Score by Class\n",
    "print(len(mt3_f1score))\n",
    "print(\"Type 1 F1 Score: {:0.2f}%\".format(mean(t1_f1score) * 100))\n",
    "print(\"VST3 F1 Score: {:0.2f}%\".format(mean(vst3_f1score) * 100))\n",
    "print(\"ST3 F1 Score: {:0.2f}%\".format(mean(st3_f1score) * 100))\n",
    "print(\"MT3 F1 Score: {:0.2f}%\".format(mean(mt3_f1score) * 100))\n",
    "print(\"WT3/4 F1 Score: {:0.2f}%\".format(mean(wt3_f1score) * 100))\n",
    "print(\"False F1 Score: {:0.2f}%\".format(mean(t0_f1score) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65217f",
   "metadata": {},
   "source": [
    "# # Train-Test Method Ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d011c96-4fed-468c-b7e4-b7164a4b00b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
